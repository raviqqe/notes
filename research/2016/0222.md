# Research notes on 2016/2/22

## The conference raking by difficulities of getting your papers passed

1. ACL
2. NAACL
3. EMNLP
4. and the others...


## The paper submission deadlines of some conferences

conference name | deadline
----------------|----------------------------------------
EMNLP 2016      | June 3, 2016 (both long and short ones)
COLING 2016     | July 15, 2016


## 日本語手書き文字データセット

The feature lists below are quoted from comments
in this article on [Qiita](http://qiita.com),
[はじめてのDeepLearning入門(Chainer) 日本語文字認識 1章\[環境構築\]]
(http://qiita.com/LichtLabo/items/94063ee9d66263d1911a).

1. [環境研究所 日本語OCR用データセット](http://naturecode.ddo.jp/home/)
  - 有料 (depending on the character sets you want)
  - 活字中心
  - 商用可でOSS公開などが容易
  - 第1,第2水準漢字あり
  - ギリシャ文字の一部、数学記号などあり
  - PNGファイルで前処理等が楽

2. [ETL文字データベース](http://etlcdb.db.aist.go.jp/?lang=ja)
  - 無料（ユーザー登録すればすぐに利用可能）
  - 手書き中心
  - 文字1種類あたりのデータが多い
  - 線の太さが大体揃っている（太さのバリエーションが少ない）
  - 拗音、ゐ, ゑがない
  - カタカナ、算用数字がない
  - 商用不可でOSS公開などが難しい
  - データ内に隣の文字の一部が入り混んでいることがよくある


## The future planning

1. Read the papers below
  - [Character-Aware Neural Language Models](http://arxiv.org/pdf/1508.06615.pdf),
    Yoon Kim et al.
  - [Not All Contexts Are Created Equal: Better Word Representations with Variable Attention]
    (http://www.cs.cmu.edu/~lingwang/papers/emnlp2015-2.pdf),
    Wang Ling et al.
  - [Character-level Convolutional Networks for Text Classification]
    (http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf),
    Xiang Zhang et al.
  - [Document Modeling with Gated Recurrent Neural Network for Sentiment Classification]
    (http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP167.pdf),
    Duyu Tang et al.
  - [Recurrent Memory Network for Language Modeling](http://arxiv.org/abs/1601.01272),
    Ke Tran et al.
  - [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation]
    (http://arxiv.org/abs/1508.02096),
    Wang Ling et al.
  - [End-To-End Memory Networks](http://arxiv.org/abs/1503.08895),
    SainBayar Sukhbaatar et al.
  - And other papers related to combinatory use of RNNs and attention

2. Determine the baseline method and implement it,
   and then test it so that it achives the same performance
   as its original paper shows.
  - The baseline method can be "Character-Aware Neural Language Models" so far.

3. Extend it
  - Construct character features out of their font images
    That will works well especially with kanjis hopefully.
  - Add attentions to the Recurrent Neural Networks (RNNs)
    between characters, words, and sentneces.
