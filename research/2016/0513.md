# Research notes

## What to do next?

- 上位頻度の単語で文字数を決める
- ある単語長でなん％の単語が切られるのか
- 英語には簡単な言語の由来と難しいのがある
- 単語長、文長、文書長で頻度分布を調べどれくらいで切ったらいいか見る
- 句を単語と文の間に挟む
- 99%超えているぐらいのを一回Accuracy測る
- 単語は全部入れたほうがいい?


## char2word2sent2doc fails with GPU's OOM kill

As of `b3176460421`, May 13, 2016, char2word2sent2doc model fails
with a little big embedding sizes (~64 for each NL expressions).
While I'm not sure that this is caused
by the parameter size (complexity?) of my model,
I tried to figure out which code consume so much memory
tampering with `model.py`.

With the revision `b3176460421`, in `with tf.variable_scope("model")` block,
there are 3 parts of code each of which starts with
`output_layer = ...`, `loss, accuracy, ...`,
and `training = tf.train.AdamOptimizer ...`.
I wrapped them and some combination of them
with `with tf.device("/cpu:0"): ...` block and check out if it works well.
Here is the result.

output_layer = ... | loss, ... | training = ... | works or not
-|-|-|-
cpu | cpu | cpu | yes
